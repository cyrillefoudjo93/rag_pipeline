{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "import fitz  # PyMuPDF for extracting text from PDFs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import certifi\n",
    "import spacy\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure NLTK and Dependencies are Installed: Make sure you have NLTK installed correctly in the current virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cyrillekonzeu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/cyrillekonzeu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cyrillekonzeu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/cyrillekonzeu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "# Set the SSL context\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Now download some dependencies for NLTK\n",
    "try:\n",
    "    nltk.download('punkt')      # download the Punkt tokenizer models for sentence tokenization\n",
    "    nltk.download('punkt_tab')  # specifically download punkt_tab for tokenization of tab-delimited files (e.g., TSV)\n",
    "    nltk.download('wordnet')    # download the WordNet corpus for synsets and lemmas\n",
    "    nltk.download('omw-1.4')    # download the Open Multilingual WordNet (OMW) corpus for synsets in multiple languages\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Extracts text from all PDF files in the folder called \"pdfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdfs(pdf_folder):\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(pdf_folder, filename)\n",
    "            with fitz.open(file_path) as pdf_file:\n",
    "                text = \"\"\n",
    "                for page in pdf_file:\n",
    "                    text += page.get_text()\n",
    "                all_texts.append(text)\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Breaks the extracted text into paragraphs or sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_into_paragraphs(text, min_length=50):\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    meaningful_paragraphs = [p.strip() for p in paragraphs if len(p.strip()) > min_length]\n",
    "    return meaningful_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Creates a vector database for the given paragraphs using TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_database(paragraphs):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(paragraphs).toarray()\n",
    "\n",
    "    # Create FAISS index\n",
    "    dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # Using L2 distance\n",
    "    index.add(np.array(vectors).astype('float32'))  # Convert to float32 for FAISS\n",
    "\n",
    "    return index, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Summarizes the given text using the TextRank algorithm from the `sumy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, sentence_count=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"german\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentence_count)\n",
    "    summarized_response = \" \".join(str(sentence) for sentence in summary)\n",
    "    return summarized_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Queries the vector database and retrieves the most relevant paragraphs based on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector_db(query, index, vectorizer):\n",
    "    query_vector = vectorizer.transform([query]).toarray().astype('float32')\n",
    "    _, indices = index.search(query_vector, k=5)  # Retrieve top 5 paragraphs\n",
    "    return indices.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a function that takes a text query, vectorizes it, and retrieves the most relevant passages from the vector database. Combine the retrieved passages to create a coherent response to the query. Make sure that the response is meaningful and understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(pdf_folder, query, sentence_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Step 1: Extract text from the PDF documents\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     all_texts \u001b[38;5;241m=\u001b[39m extract_text_from_pdfs(pdf_folder)\n",
      "File \u001b[0;32m~/Documents/RAG Pipeline/.venv/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/RAG Pipeline/.venv/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/RAG Pipeline/.venv/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m~/Documents/RAG Pipeline/.venv/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m~/Documents/RAG Pipeline/.venv/lib/python3.12/site-packages/gensim/matutils.py:1034\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[0;32m~/Documents/RAG Pipeline/.venv/lib/python3.12/site-packages/gensim/_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "\n",
    "def main(pdf_folder, query, sentence_count=3):\n",
    "    # Step 1: Extract text from the PDF documents\n",
    "    all_texts = extract_text_from_pdfs(pdf_folder)\n",
    "\n",
    "    # Combine all extracted texts into a single string\n",
    "    combined_text = \" \".join(all_texts)\n",
    "\n",
    "    # Step 2: Break the extracted text into meaningful paragraphs\n",
    "    paragraphs = break_into_paragraphs(combined_text)\n",
    "\n",
    "    # Step 3: Create a vector database for the paragraphs\n",
    "    index, vectorizer = create_vector_database(paragraphs)\n",
    "\n",
    "    # Step 4: Query the vector database\n",
    "    print(f\"Querying for: {query}\")\n",
    "    indices = query_vector_db(query, index, vectorizer)\n",
    "\n",
    "    # Retrieve and summarize the most relevant paragraphs\n",
    "    relevant_paragraphs = [paragraphs[i] for i in indices]\n",
    "    combined_relevant_text = \" \".join(relevant_paragraphs)\n",
    "\n",
    "    # Step 5: Summarize the relevant paragraphs\n",
    "    # summarized_text = summarize_text(combined_relevant_text, sentence_count=sentence_count)\n",
    "    summarized_text = summarize(combined_relevant_text, ratio=0.2)\n",
    "    \n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for: Wie hoch ist die Grundzulage?\n",
      "Die monatliche Rente aus der Teilkapitalverrentung muss mindestens so hoch sein, wie Die monatliche Rente aus der Teilkapitalverrentung muss mindestens so hoch sein, wie Die monatliche Rente aus der Teilkapitalverrentung muss mindestens so hoch sein, eigenbeitrag, ist die für dieses Beitragsjahr zustehende Altersvorsorgezulage (Grundzulage eigenbeitrag, ist die für dieses Beitragsjahr zustehende Altersvorsorgezulage (Grundzulage\n"
     ]
    }
   ],
   "source": [
    "pdf_folder = \"./pdfs\"\n",
    "query = \"Wie hoch ist die Grundzulage?\"\n",
    "response = main(pdf_folder, query, sentence_count=3)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
